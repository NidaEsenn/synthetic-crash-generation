{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Crash Scene Generation Pipeline — Demo\n",
    "\n",
    "**End-to-end pipeline:** Crash report text → Structured scene → Depth-conditioned image → Video\n",
    "\n",
    "This notebook demonstrates the full multi-model pipeline running on a free Google Colab T4 GPU.\n",
    "\n",
    "### Pipeline Stages:\n",
    "1. **Parse** — Groq LLM extracts structured scene representation from crash report\n",
    "2. **Depth** — Depth Anything V2 + programmatic manipulation creates depth conditioning\n",
    "3. **Image** — ControlNet + SDXL generates depth-conditioned dashcam image\n",
    "4. **Video** — Wan2.1 generates 5-second dashcam video\n",
    "5. **Evaluate** — CLIP score + YOLO verification measures quality\n",
    "\n",
    "### VRAM Management:\n",
    "Models are loaded/unloaded sequentially to fit within T4's 15GB VRAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Mount Google Drive\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\n# Navigate to project\n%cd /content/drive/MyDrive/tesla_crash_synth\n\n# Install dependencies\n!pip install -q torch torchvision transformers diffusers accelerate\n!pip install -q openai pydantic python-dotenv pillow numpy opencv-python scipy\n!pip install -q ultralytics  # For YOLO evaluation\n!pip install -e . -q"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set API key (paste your Groq key here or load from .env)\n",
    "import os\n",
    "os.environ[\"GROQ_API_KEY\"] = \"\"  # <-- paste your key if not using .env\n",
    "\n",
    "# Verify GPU\n",
    "import torch\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"VRAM: {torch.cuda.get_device_properties(0).total_mem / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Scenarios\n",
    "\n",
    "Four diverse crash scenarios testing different conditions:\n",
    "- Weather (rain, night, clear)\n",
    "- Road type (highway, intersection, residential)\n",
    "- Incident type (hydroplane, pedestrian, side-impact, rear-end)\n",
    "- Object types (guardrail, pedestrian, vehicle, truck)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scenarios = {\n",
    "    \"wet_highway\": \"Vehicle traveling 45mph on wet highway in heavy rain, hydroplaned and hit guardrail on the left side\",\n",
    "    \"night_pedestrian\": \"Pedestrian crossed outside crosswalk at night on a residential street, struck by vehicle going 25mph, pedestrian was wearing dark clothing\",\n",
    "    \"intersection_crash\": \"Vehicle ran red light at 35mph at urban intersection, T-bone side impact with cross traffic vehicle going 30mph\",\n",
    "    \"rear_end_highway\": \"Car rear-ended stopped truck at highway on-ramp in clear weather, going approximately 40mph, truck was loaded with cargo\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 1: Parse All Scenarios\n",
    "\n",
    "The enhanced parser extracts:\n",
    "- Basic crash fields (speed, weather, incident type)\n",
    "- **Scene objects with spatial positions** (for depth map manipulation)\n",
    "- **Temporal description** (for video generation)\n",
    "- Rich image prompt (for diffusion model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.parser import LLMParser\n",
    "\n",
    "parser = LLMParser()\n",
    "parsed_scenarios = {}\n",
    "\n",
    "for name, report in test_scenarios.items():\n",
    "    print(f\"\\n--- Parsing: {name} ---\")\n",
    "    scenario = parser.parse(report)\n",
    "    parsed_scenarios[name] = scenario\n",
    "    \n",
    "    print(f\"  Incident: {scenario.incident_type}\")\n",
    "    print(f\"  Weather: {scenario.weather}, Lighting: {scenario.lighting}\")\n",
    "    print(f\"  Road: {scenario.road_type}, Condition: {scenario.road_condition}\")\n",
    "    print(f\"  Scene objects ({len(scenario.scene_objects)}):\")\n",
    "    for obj in scenario.scene_objects:\n",
    "        print(f\"    - {obj.type}: {obj.distance_m}m, lateral={obj.lateral_position}, action={obj.action}\")\n",
    "    print(f\"  Temporal: {scenario.temporal_description[:120]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 2: Depth Map Generation\n",
    "\n",
    "For each scenario, we:\n",
    "1. Create a base depth map matching the road type (perspective geometry)\n",
    "2. **Programmatically manipulate** the depth map using scene object positions\n",
    "\n",
    "This is NOT just running a model — we're applying geometric reasoning to place objects at correct distances for ControlNet conditioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.vram_manager import VRAMManager\n",
    "from utils.depth_generator import DepthGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "vram = VRAMManager()\n",
    "depth_gen = DepthGenerator(vram_manager=vram)\n",
    "\n",
    "depth_maps = {}\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "\n",
    "for i, (name, scenario) in enumerate(parsed_scenarios.items()):\n",
    "    # Base depth (road geometry only)\n",
    "    base_depth = depth_gen.create_base_depth(scenario.road_type)\n",
    "    \n",
    "    # Manipulated depth (with objects placed at correct distances)\n",
    "    obj_dicts = [obj.model_dump() for obj in scenario.scene_objects]\n",
    "    manipulated = depth_gen.manipulate_depth(base_depth, obj_dicts) if obj_dicts else base_depth\n",
    "    \n",
    "    depth_img = depth_gen.depth_to_pil(manipulated)\n",
    "    depth_maps[name] = depth_img\n",
    "    depth_img.save(f\"outputs/{name}_depth.png\")\n",
    "    \n",
    "    # Visualize base vs manipulated\n",
    "    axes[0, i].imshow(base_depth, cmap='viridis')\n",
    "    axes[0, i].set_title(f\"{name}\\n(base depth)\")\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    axes[1, i].imshow(manipulated, cmap='viridis')\n",
    "    axes[1, i].set_title(f\"{name}\\n(+ {len(obj_dicts)} objects)\")\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle(\"Depth Maps: Base Geometry (top) vs Object-Manipulated (bottom)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs/depth_comparison.png\", dpi=150)\n",
    "plt.show()\n",
    "print(\"Depth maps generated for all scenarios\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3: ControlNet Image Generation\n",
    "\n",
    "Generate depth-conditioned dashcam images using ControlNet + SDXL.\n",
    "\n",
    "**Key improvement over raw SDXL:** The depth map tells the model WHERE objects should appear in 3D space, producing better spatial accuracy for pedestrians, vehicles, and road elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from utils.controlnet_generator import ControlNetGenerator\n\n# Unload depth model before loading ControlNet (VRAM management)\ndepth_gen.unload_model()\n\ncn_gen = ControlNetGenerator(vram_manager=vram)\ngenerated_images = {}\n\nfor name, scenario in parsed_scenarios.items():\n    print(f\"\\nGenerating: {name}\")\n    image = cn_gen.generate_from_scenario(\n        scenario=scenario,\n        depth_image=depth_maps[name],\n        controlnet_conditioning_scale=0.25,  # lower = less tiling from programmatic depth\n    )\n    generated_images[name] = image\n    image.save(f\"outputs/{name}_image.png\")\n    print(f\"  Saved: outputs/{name}_image.png\")\n\nvram.snapshot(\"after_all_images\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all generated images\n",
    "fig, axes = plt.subplots(1, 4, figsize=(24, 6))\n",
    "for i, (name, img) in enumerate(generated_images.items()):\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].set_title(name.replace('_', ' ').title(), fontsize=12)\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle(\"Generated Dashcam Images (ControlNet + SDXL)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs/all_images.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 3b: Crash-Moment Keyframes\n",
    "\n",
    "Attempt to generate temporal keyframes: \"3 seconds before\", \"1 second before\", and \"impact moment\".\n",
    "\n",
    "**Honest expectation:** Pre-crash keyframes should look reasonable. The impact moment will likely be imperfect — this is documented as part of the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate keyframes for one scenario (to save time)\n",
    "demo_scenario = \"wet_highway\"\n",
    "print(f\"Generating temporal keyframes for: {demo_scenario}\")\n",
    "\n",
    "keyframes = cn_gen.generate_crash_keyframes(\n",
    "    scenario=parsed_scenarios[demo_scenario],\n",
    "    depth_image=depth_maps[demo_scenario],\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "for i, (timepoint, kf_img) in enumerate(keyframes.items()):\n",
    "    axes[i].imshow(kf_img)\n",
    "    axes[i].set_title(timepoint.replace('_', ' '), fontsize=12)\n",
    "    axes[i].axis('off')\n",
    "    kf_img.save(f\"outputs/{demo_scenario}_{timepoint}.png\")\n",
    "\n",
    "plt.suptitle(f\"Temporal Keyframes: {demo_scenario.replace('_', ' ').title()}\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs/keyframes.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Stage 4: Video Generation (SVD)\n\nAnimate a generated dashcam image into a 3.5-second video using Stable Video Diffusion.\n\n**Why SVD instead of Wan2.1:** Wan2.1 crashes Colab free tier's 12GB system RAM. SVD is image-to-video (~6GB VRAM), lighter, and animates our ControlNet output directly.\n\n**Important:** ControlNet must be fully unloaded first to free memory."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import gc\nimport torch\nfrom utils.video_generator import VideoGenerator\n\n# Fully unload ControlNet to free both VRAM and system RAM\ncn_gen.unload_model()\ndel cn_gen\ngc.collect()\ntorch.cuda.empty_cache()\n\nprint(f\"VRAM after cleanup: {torch.cuda.memory_allocated()/1024**3:.1f} GB\")\n\n# Load SVD (image-to-video, ~6GB)\nvideo_gen = VideoGenerator(vram_manager=vram)\n\n# Animate the best generated image\ndemo_name = \"wet_highway\"\nprint(f\"Generating video for: {demo_name}\")\n\nframes = video_gen.generate(\n    image=generated_images[demo_name],\n    num_frames=25,  # 25 frames = ~3.5s at 7fps\n)\nvideo_gen.export_video(frames, f\"outputs/{demo_name}_video.mp4\")\n\nvideo_gen.unload_model()\nprint(\"Video generation complete\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage 5: Evaluation\n",
    "\n",
    "Quantitative quality assessment:\n",
    "- **CLIP score** — How well does the generated image match the text prompt?\n",
    "- **YOLO verification** — Did the model actually render the objects we requested?\n",
    "- **Quality grade** — Human-readable assessment based on metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.evaluator import ScenarioEvaluator\n",
    "import json\n",
    "\n",
    "evaluator = ScenarioEvaluator()\n",
    "\n",
    "eval_results = {}\n",
    "for name, scenario in parsed_scenarios.items():\n",
    "    print(f\"\\nEvaluating: {name}\")\n",
    "    report = evaluator.evaluate_scenario(\n",
    "        image=generated_images[name],\n",
    "        scenario=scenario,\n",
    "        label=\"controlnet_depth\",\n",
    "    )\n",
    "    eval_results[name] = report\n",
    "    \n",
    "    print(f\"  CLIP score: {report['clip_score']}\")\n",
    "    print(f\"  Objects found: {report['object_verification']['found']}\")\n",
    "    print(f\"  Objects missing: {report['object_verification']['missing']}\")\n",
    "    print(f\"  Detection rate: {report['object_verification']['detection_rate']}\")\n",
    "    print(f\"  Quality: {report['quality_assessment']}\")\n",
    "\n",
    "# Save all evaluations\n",
    "with open(\"outputs/evaluation_report.json\", \"w\") as f:\n",
    "    json.dump(eval_results, f, indent=2, default=str)\n",
    "print(\"\\nFull evaluation saved to outputs/evaluation_report.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VRAM Usage Report\n",
    "\n",
    "Profiling GPU memory across all pipeline stages — demonstrates VRAM-aware orchestration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vram.print_report()\n",
    "\n",
    "# Visualize VRAM usage over time\n",
    "report = vram.report()\n",
    "stages = [s[\"stage\"] for s in report[\"stages\"]]\n",
    "peaks = [s[\"peak_mb\"] for s in report[\"stages\"]]\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.bar(range(len(stages)), peaks, color='steelblue')\n",
    "plt.axhline(y=15360, color='red', linestyle='--', label='T4 VRAM limit (15GB)')\n",
    "plt.xticks(range(len(stages)), stages, rotation=45, ha='right', fontsize=8)\n",
    "plt.ylabel('Peak VRAM (MB)')\n",
    "plt.title('GPU Memory Usage Across Pipeline Stages')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"outputs/vram_usage.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Failure Analysis\n",
    "\n",
    "### What Worked\n",
    "- Depth conditioning improves spatial accuracy over raw SDXL\n",
    "- Scene object parsing from crash reports → structured depth manipulation\n",
    "- Video generation produces temporally consistent dashcam footage\n",
    "- VRAM-aware sequential loading keeps pipeline within T4 limits\n",
    "\n",
    "### Known Limitations\n",
    "- **Crash moment quality**: Diffusion models can't reliably render physical collision dynamics\n",
    "- **Small objects**: Pedestrians at distance remain hard to render accurately\n",
    "- **Temporal consistency**: T2V doesn't guarantee consistency with the generated still image\n",
    "- **Object detection gap**: YOLO can't verify all object types (guardrails, debris)\n",
    "\n",
    "### ML Concepts Demonstrated\n",
    "- Diffusion conditioning theory (depth maps as geometric priors)\n",
    "- VRAM-aware multi-model pipeline orchestration\n",
    "- LLM-structured scene understanding (CrashAgent-inspired)\n",
    "- Programmatic depth manipulation (geometric reasoning)\n",
    "- Video diffusion (Wan2.1)\n",
    "- Automated evaluation methodology (CLIP, YOLO)\n",
    "- Honest failure analysis with quantitative metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary table\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FINAL EVALUATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Scenario':<25} {'CLIP':>8} {'Det. Rate':>10} {'Quality':>30}\")\n",
    "print(\"-\"*70)\n",
    "for name, report in eval_results.items():\n",
    "    clip = report['clip_score']\n",
    "    det = report['object_verification']['detection_rate']\n",
    "    qual = report['quality_assessment'].split(' — ')[0]\n",
    "    print(f\"{name:<25} {clip:>8.4f} {det:>10.2f} {qual:>30}\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_minor_version": 0,
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}